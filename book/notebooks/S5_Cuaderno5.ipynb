{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb090615-4bdd-4206-af7a-9a30bfe77d63",
   "metadata": {},
   "source": [
    "# Cuaderno 5: Perceptrones\n",
    "\n",
    "Los *perceptrones* fueron introducidos por Frank Rosenblatt entre 1957 y 1962 como una familia de modelos teóricos y experimentales de redes neuronales artificiales. En su momento generaron mucho entusiasmo, pero también bastante controversia sobre sus alcances. A partir de ese trabajo surgieron modelos posteriores más abstractos y matemáticamente formales.  \n",
    "\n",
    "El antecedente directo fue la **neurona de McCulloch y Pitts** (1943), que proponía una neurona muy simplificada, pensada como un elemento de cálculo lógico. Esa idea abrió la puerta a imaginar redes de unidades elementales capaces de computar funciones más complejas.  \n",
    "\n",
    "Hoy en día, cuando decimos *perceptrón simple* solemos referirnos a un clasificador lineal de una sola capa (con una función de activación tipo escalón o sigmoide), y con *perceptrón multicapa* (MLP, *multilayer perceptron*) a una red de retroalimentación (*feedforward*) con al menos una capa oculta y entrenada mediante retropropagación (un algoritmo que recién se popularizó en los años 80).  \n",
    "\n",
    "Aunque la inspiración inicial vino de la biología, las redes neuronales modernas abstraen mucho esos detalles y se piensan más como funciones matemáticas: cada capa aplica una transformación lineal seguida de una no linealidad. En ese sentido, comparten gran parte de la matemática con la **regresión logística**.  \n",
    "\n",
    "La diferencia es que las redes neuronales son más poderosas: se ha demostrado que un perceptrón multicapa con una sola capa oculta y suficientes neuronas puede aproximar cualquier función continua en un dominio acotado (esto se conoce como el **teorema de aproximación universal**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862be1c5-a903-4de7-bc2a-334db71f0b68",
   "metadata": {},
   "source": [
    "## Configuración\n",
    "\n",
    "Como es usual, comenzamos importando las librerías que vamos a utilizar. Corré las celdas en esta sección para cargar las funciones que vamos a usar en el cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49352033-e3f5-4724-8f3a-35a2c643805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675ef8d-0aee-4ea3-a49a-4d55b42e1f4d",
   "metadata": {},
   "source": [
    "### Funciones de graficado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a6dc8-a61b-4ecc-a0c2-69f75d787151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_errores(t, errors, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.plot(t, errors)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.set_xlabel(\"Iteración\")\n",
    "    ax.set_ylabel(\"Error\")\n",
    "    ax.axhline(0, color='r')\n",
    "\n",
    "def visualizar_pesos(W, b, X=None, Y=None, ax=None, titulo=\"Límites de decisión\"):\n",
    "    \"\"\"\n",
    "    W: (2,) o (k,2)  pesos [w1, w2] por neurona\n",
    "    b: escalar o (k,1)/(k,)  sesgo por neurona\n",
    "    X, Y: opcionales para plotear puntos del dataset\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    # Normalizar formas\n",
    "    W = np.atleast_2d(W)           # -> (k,2)\n",
    "    if np.isscalar(b):\n",
    "        b = np.array([b]*len(W)).reshape(-1, 1)  # -> (k,1)\n",
    "    else:\n",
    "        b = np.atleast_2d(b)\n",
    "        if b.shape[0] != W.shape[0]:\n",
    "            # si vino transpuesto, intentamos acomodar\n",
    "            if b.shape[1] == W.shape[0]:\n",
    "                b = b.T\n",
    "        if b.shape[1] != 1:\n",
    "            b = b.reshape(-1, 1)\n",
    "\n",
    "    # Rango amigable para entradas binarias\n",
    "    x_vals = np.linspace(-0.2, 1.2, 400)\n",
    "\n",
    "    # Dibujar una recta por neurona: w1*x + w2*y + b = 0  -> y = -(w1*x + b)/w2\n",
    "    for i, (wi, bi) in enumerate(zip(W, b)):\n",
    "        if abs(wi[1]) > 1e-12:\n",
    "            y_vals = -(wi[0]*x_vals + bi.item()) / wi[1]\n",
    "            ax.plot(x_vals, y_vals, linestyle='--', label=f\"neurona {i+1}\")\n",
    "        else:\n",
    "            # frontera vertical: x = -b/w1\n",
    "            x0 = -bi.item() / wi[0]\n",
    "            ax.axvline(x0, linestyle='--', label=f\"neurona {i+1}\")\n",
    "\n",
    "    # Puntos del dataset (si se pasan)\n",
    "    if X is not None and Y is not None:\n",
    "        yy = Y.ravel()\n",
    "        ax.scatter(X[0, yy==0], X[1, yy==0], marker='o', label='Clase 0')\n",
    "        ax.scatter(X[0, yy==1], X[1, yy==1], marker='s', label='Clase 1')\n",
    "\n",
    "    ax.set_xlim(-0.2, 1.2)\n",
    "    ax.set_ylim(-0.2, 1.2)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.axvline(0, color='black', linewidth=0.5)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title(titulo)\n",
    "    ax.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775aa95-9846-4df9-b3ce-0c1255acf866",
   "metadata": {},
   "source": [
    "## Perceptrón simple\n",
    "\n",
    "El primer problema de nuestro perceptrón va a ser el de aprender a imitar una compuerta lógica **OR**.  \n",
    "Las compuertas OR son dispositivos electrónicos con una función booleana que devuelve 1 cuando al menos una de sus dos entradas está prendida.  \n",
    "\n",
    "El objetivo del perceptrón va a ser, a partir de dos entradas binarias y una salida esperada, ajustar los pesos de su capa de activación de manera que, tras entrenarse varias veces, siempre pueda reproducir la función booleana OR.  \n",
    "\n",
    "En este caso, vamos a entrenar un perceptrón de dos nodos de entrada y uno de salida con los siguientes pares de ejemplo:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathbf{X}_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\; \\mathbf{Y}_1 = \\begin{pmatrix} 0 \\end{pmatrix} \\\\\n",
    "\\mathbf{X}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\; \\mathbf{Y}_2 = \\begin{pmatrix} 1 \\end{pmatrix} \\\\\n",
    "\\mathbf{X}_3 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\; \\mathbf{Y}_3 = \\begin{pmatrix} 1 \\end{pmatrix} \\\\\n",
    "\\mathbf{X}_4 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\; \\mathbf{Y}_4 = \\begin{pmatrix} 1 \\end{pmatrix}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Para expresarlo de manera más compacta, podemos reunir todas las combinaciones en una matriz de entradas $\\mathbf{X}$ y un vector de salidas $\\mathbf{Y}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 1 & 1 \\\\\n",
    "0 & 1 & 0 & 1\n",
    "\\end{pmatrix}, \\quad\n",
    "\\mathbf{Y} =\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 1 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "De esta manera el perceptrón ve **todas las posibles combinaciones** de las dos entradas y la salida esperada correspondiente.\n",
    "\n",
    "### Componentes del perceptrón\n",
    "\n",
    "Un perceptrón simple tiene tres elementos básicos:\n",
    "\n",
    "- **Entradas:** los valores que alimentan al perceptrón (acá, dos bits de entrada).  \n",
    "- **Pesos:** cada entrada tiene un peso asociado que indica su importancia relativa. Estos pesos se ajustan durante el entrenamiento.  \n",
    "- **Función de activación:** después de calcular la suma ponderada $z$, se aplica una función que decide si la neurona se “activa” o no.\n",
    "\n",
    "### Modelo matemático\n",
    "\n",
    "Podemos modelar los pesos con un vector columna $\\mathbf{w} = (w_1, w_2)^\\top$.  \n",
    "Para un ejemplo de entrada $\\mathbf{x} = (x_1, x_2)^\\top$, la salida lineal del perceptrón es:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "donde $b$ es el **sesgo**, que permite desplazar la frontera de decisión.\n",
    "\n",
    "Finalmente, este valor $z$ se pasa por una función de activación no lineal. En el perceptrón original de Rosenblatt se usaba una **función escalón** (devuelve 0 o 1). En versiones modernas se usa a menudo la **función sigmoide**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)\n",
    "$$\n",
    "\n",
    "donde $\\sigma$ es la función sigmoide logística. El valor $\\hat{y}$ es la salida del perceptrón y se compara con la salida esperada $Y$ para ajustar los pesos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236742b6-60eb-4e3e-bee8-0bc832c831fd",
   "metadata": {},
   "source": [
    "### Función de activación $\\sigma$\n",
    "\n",
    "La **función de activación** transforma el resultado de multiplicar las entradas por los pesos y sumar el sesgo en un valor no lineal. Esto es clave cuando queremos que el perceptrón tome una **decisión binaria**, como en problemas de clasificación.  \n",
    "\n",
    "Una función de activación muy usada es la **sigmoide** (aunque hay muchas otras):\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-\\beta x + b}}\n",
    "$$\n",
    "\n",
    "Esta función devuelve valores en el rango $(0,1)$, nunca exactamente 0 o 1, pero muy cercanos. Así podemos interpretar la salida como una **probabilidad** de que la neurona se active.  \n",
    "\n",
    "Ejecutá la celda siguiente para verla en acción, y pensá:  \n",
    "\n",
    "- ¿Qué ocurre cuando aumenta $\\beta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac590a79-2624-43a3-bee3-91d847102439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, beta, b):\n",
    "    return 1 / (1 + np.exp(-beta * x + b))\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "\n",
    "@widgets.interact(beta=(0, 10, 0.01), b=(-10, 10, 0.5))\n",
    "def simulate(beta, b=0):\n",
    "    plt.plot(x, sigmoid(x, beta, b))\n",
    "    plt.vlines(x=0, ymin=0, ymax=1, color='r', linestyles='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141840aa-8cfc-4fc1-bf73-71c9b4657491",
   "metadata": {},
   "source": [
    "### Compuerta OR\n",
    "\n",
    "El entrenamiento del perceptrón se realiza utilizando un algoritmo conocido como regla de aprendizaje del perceptrón. Este ajusta los pesos basándose en el error de las predicciones, buscando minimizar la diferencia entre las salidas predichas y las reales. En la clase vimos como combinando el aprendizaje Hebbiano con la regla de la cadena podemos ir aproximandonos a los pesos que buscamos.\n",
    "\n",
    "Veamos la implementación de un perceptrón simple y entrenémoslo para detectar la compuerta OR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d36289-0d6a-4b10-b0a1-9bf8aa00adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umbral de activación\n",
    "threshold = 0.5\n",
    "\n",
    "# Definimos el modelo lineal que queremos resolver\n",
    "def z(X, W, b):\n",
    "    return W.dot(X) + b\n",
    "\n",
    "# Definimos la función de activación\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def y(X, W, b):\n",
    "    return sigma(z(X, W, b))\n",
    "\n",
    "# Definimos la derivada de la función de activación dy/dw\n",
    "def dg(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "# Definimos la función que calcula el error\n",
    "def loss(x, x_pred):\n",
    "    return x - x_pred\n",
    "\n",
    "# Definimos la funcion que entrena un conjunto de datos de entrada y salida y grafica el error cometido\n",
    "# en cada salto de tiempo.\n",
    "# X: Las entradas del conjunto de entrenamiento\n",
    "# Y: Las salidas esperadas del conjunto de entrenamiento\n",
    "# alpha: Constante de aprendizaje\n",
    "# tmax: Tiempo máximo que queremos entrenar\n",
    "def perceptron_simple(X, Y, alpha=0.5, tmax=1000):\n",
    "    # Iniciamos un array que va a guardar los errores en cada paso para poder graficarlos al final\n",
    "    # y observar como va aprendiendo el perceptrón en cada paso\n",
    "    errors = []\n",
    "\n",
    "    # Fijamos la semilla para reproducir siempre los mismos valores aleatorios\n",
    "    np.random.seed(1) \n",
    "    \n",
    "    # Iniciamos la matriz con los pesos en forma aleatoria\n",
    "    W = np.random.randn(Y.shape[0], X.shape[0])\n",
    "\n",
    "    # Iniciamos el término de sesgo con un número aleatorio\n",
    "    b = np.random.uniform(-1, 1)\n",
    "    \n",
    "    # Definimos la cantidad de trials que vamos a usar\n",
    "    t = np.arange(tmax)\n",
    "\n",
    "    for step in t:\n",
    "        y_hat = y(X, W, b)\n",
    "        error = loss(Y, y_hat)\n",
    "        delta = error * dg(y_hat)\n",
    "        W = W + alpha * delta.dot(X.T)\n",
    "        b = b + alpha * delta.sum()\n",
    "\n",
    "        errors = np.append(errors, np.sum(error ** 2))\n",
    "\n",
    "    # Graficamos el error\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    visualizar_errores(t, errors, ax1)\n",
    "    visualizar_pesos(W, b, X, Y, ax2)\n",
    "    plt.show()\n",
    "\n",
    "    # Imprimimos los parámetros\n",
    "    print(\"W =\", W)\n",
    "    print(\"b =\", b)\n",
    "    \n",
    "    return lambda X: np.where(y(X, W, b) < threshold, 0, 1)\n",
    "\n",
    "# Entrenamos al perceptrón para la funcion OR\n",
    "@widgets.interact(alpha=(0, 1, 0.01), tmax=(1, 1000))\n",
    "def simulate(alpha=0.5, tmax=1000):\n",
    "    X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "    Y = np.array([[0, 1, 1, 1]])\n",
    "    perceptron_simple(X, Y, alpha, tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3c985-f545-4109-ab59-7755b41aad26",
   "metadata": {},
   "source": [
    "Ejercitamos al perceptrón con dos valores de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de9572-a541-44ca-a6e6-7697ed0356a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1, 1], \n",
    "              [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 1]])\n",
    "f = perceptron_simple(X, Y, alpha=0.1, tmax=100)\n",
    "y_hat = f(np.array([[1], [0]]))\n",
    "print(f\"Salida: {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a31997-bd19-4207-9a2a-c4c9fb16b101",
   "metadata": {},
   "source": [
    "### Compuerta AND\n",
    "\n",
    "En una forma similar, podemos entrenar al perceptrón para detectar cuando los dos nodos de entrada se activan al mismo tiempo. Esto puede ser visto como la función booleana AND. En este caso, los vectores de entrenamiento y de salida esperada pueden ser:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\pmb{X_1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} & \\pmb{Y_1} = \\begin{pmatrix} 0 \\end{pmatrix} \\\\\n",
    "\\pmb{X_2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} & \\pmb{Y_2} = \\begin{pmatrix} 0 \\end{pmatrix} \\\\\n",
    "\\pmb{X_3} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} & \\pmb{Y_3} = \\begin{pmatrix} 0\\end{pmatrix} \\\\\n",
    "\\pmb{X_4} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} & \\pmb{Y_4} = \\begin{pmatrix} 1 \\end{pmatrix}\n",
    "\\end{split}$$\n",
    "\n",
    "Para hacerlo con nuestro código, no tenemos más que entrenar al perceptrón con las nuevas matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779b809-7252-49a6-a9a9-0c2d6a4a0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1, 1], \n",
    "              [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 0, 0, 1]])\n",
    "f = perceptron_simple(X, Y, alpha=0.5, tmax=1000)\n",
    "y_hat = f(np.array([[1], [0]]))\n",
    "print(f\"Salida: {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3147778-05e7-468a-8608-3bf18768b03d",
   "metadata": {},
   "source": [
    "### Compuerta XOR\n",
    "\n",
    "Veamos ahora la compuerta XOR. Esta compuerta, también conocida como compuerta OR exclusiva, detecta cuando una entrada esta activa, pero no la otra. Para resolver este caso, los vectores de entrenamiento y de salida esperada pueden ser:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\pmb{X_1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} & \\pmb{Y_1} = \\begin{pmatrix} 0 \\end{pmatrix} \\\\\n",
    "\\pmb{X_2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} & \\pmb{Y_2} = \\begin{pmatrix} 1 \\end{pmatrix} \\\\\n",
    "\\pmb{X_3} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} & \\pmb{Y_3} = \\begin{pmatrix} 1 \\end{pmatrix} \\\\\n",
    "\\pmb{X_3} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} & \\pmb{Y_4} = \\begin{pmatrix} 0 \\end{pmatrix}\n",
    "\\end{split}$$\n",
    "\n",
    "Este problema de apariencia inocente es difícil porque no es linealmente separable. Si ponemos las cuatro posibles entradas en un plano, no hay ninguna línea recta que separe los casos que tienen que dar 1 de los casos que tienen que dar 0. \n",
    "Vamos a comparar a una red de 2 entradas y una salida que entrenamos con la regla delta y a una red que como la presentada más arriba, además de las 2 entradas y la salida tiene dos unidades en la capa oculta.\n",
    "\n",
    "*Ejercicio: Entrenar al perceptrón con estos vectores de entrenamiento y de salida esperados.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606db687-172e-4e28-8849-d48037e3d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos al perceptrón para la funcion XOR\n",
    "X = np.array([[0, 0, 1, 1], \n",
    "              [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "f = perceptron_simple(X, Y, alpha=0.5, tmax=1000)\n",
    "y_hat = f(np.array([[1], [0]]))\n",
    "print(f\"Salida: {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277bed6-413b-4055-b040-7534562379f5",
   "metadata": {},
   "source": [
    "No es posible construir un perceptrón de una sola capa que compute la operación lógica **XOR**. En su libro *Perceptrons* (1969), Marvin Minsky y Seymour Papert demostraron varios teoremas sobre las limitaciones de estos modelos, entre ellos que no podían implementar funciones como la exclusión OR (XOR), la paridad o la conectividad.\n",
    "\n",
    "La intuición es que un perceptrón simple es un **clasificador lineal**. Para dos entradas, la ecuación\n",
    "\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = 0\n",
    "$$\n",
    "\n",
    "describe una recta que actúa como frontera de decisión: todos los puntos de un lado se clasifican como 0 y todos los del otro como 1.  \n",
    "\n",
    "El problema es que la función XOR **no es linealmente separable**: no existe una sola recta que divida el plano en las dos categorías correctas. Para lograrlo hacen falta múltiples rectas combinadas, es decir, **una red con al menos una capa oculta** (un perceptrón multicapa).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d3535-f6b9-4b38-8d40-11cd1e9f7b0a",
   "metadata": {},
   "source": [
    "## Perceptrón multicapa\n",
    "\n",
    "Los resultados de Minsky y Papert (1969) se aplicaban únicamente a perceptrones de **una sola capa**.  \n",
    "En 1974, Paul Werbos presentó en su tesis doctoral un procedimiento general para ajustar adaptativamente los pesos de un sistema no lineal diferenciable, calculando derivadas desde las salidas hacia las entradas.  \n",
    "Este algoritmo, conocido como **retropropagación del error** (*backpropagation*), permite entrenar perceptrones multicapa mediante descenso por gradiente y muestras de entrenamiento.  \n",
    "El trabajo se difundió masivamente recién en 1986, gracias a Rumelhart, Hinton y Williams.  \n",
    "\n",
    "El **perceptrón multicapa (MLP)** agrega una o más **capas ocultas** (\\(h\\), por *hidden*) conformadas por un número arbitrario de nodos ubicados entre la capa de entrada y la de salida.  \n",
    "Los nodos de salida ya no reciben directamente las entradas originales, sino las activaciones de la capa oculta.  \n",
    "\n",
    "Un MLP con una sola capa oculta debe aprender dos matrices de pesos y dos sesgos:  \n",
    "\n",
    "- \\(W_h, b_h\\): conectan la entrada con la capa oculta.  \n",
    "- \\(W_y, b_y\\): conectan la capa oculta con la salida.  \n",
    "\n",
    "De esta forma, la salida de la red se define como:\n",
    "\n",
    "$$\n",
    "\\hat{y} \\;=\\; \\sigma\\!\\big(W_y \\, \\sigma(W_h X + b_h) + b_y\\big)\n",
    "$$\n",
    "\n",
    "donde \\(\\sigma\\) es la función de activación (sigmoide, tanh, ReLU u otra).  \n",
    "La retropropagación permite ajustar estos parámetros para que la salida \\(\\hat{y}\\) se aproxime cada vez más a la salida esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934f3fd-1ada-4b13-84fe-36e37fe1224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la funcion que entrena un conjunto de datos de entrada y salida y grafica el error cometido\n",
    "# en cada salto de tiempo.\n",
    "# X: Las entradas del conjunto de entrenamiento\n",
    "# Y: Las salidas esperadas del conjunto de entrenamiento\n",
    "# nodos_capa_oculta: Cantidad de nodos en la capa oculta\n",
    "# tmax: Tiempo máximo que queremos entrenar\n",
    "# alpha: Constante de aprendizaje\n",
    "def perceptron_multicapa(X, Y, nodos_capa_oculta=2, alpha=0.5, tmax=1000):\n",
    "    # Iniciamos un array que va a guardar los errores en cada paso para poder graficarlos al final\n",
    "    # y observar como va aprendiendo el perceptrón en cada paso\n",
    "    errors = []\n",
    "\n",
    "    # Fijamos la semilla para reproducir siempre los mismos valores aleatorios\n",
    "    np.random.seed(1) \n",
    "\n",
    "    # Iniciamos la matriz con los pesos de la capa de salida en forma aleatoria\n",
    "    W_y = np.random.randn(Y.shape[0], nodos_capa_oculta)\n",
    "\n",
    "    # Iniciamos la matriz con los pesos de la capa oculta en forma aleatoria\n",
    "    W_h = np.random.randn(nodos_capa_oculta, X.shape[0])\n",
    "\n",
    "    # Iniciamos los términos de sesgo para la capta de salida y la capa oculta con números aleatorios\n",
    "    b_y = np.random.uniform(-1, 1)\n",
    "    b_h = np.random.uniform(-1, 1, size=(nodos_capa_oculta, 1))\n",
    "\n",
    "    # Definimos la cantidad de trials que vamos a usar\n",
    "    t = np.arange(tmax)\n",
    "\n",
    "    for step in t:       \n",
    "        h = y(X, W_h, b_h)\n",
    "        h_hat = y(X, W_h, b_h)\n",
    "        \n",
    "        y_hat = y(h_hat, W_y, b_y)\n",
    "\n",
    "        error = loss(Y, y_hat)\n",
    "        \n",
    "        delta_y = error * dg(y_hat)\n",
    "        delta_h = W_y.T.dot(delta_y) * dg(h_hat)\n",
    "        \n",
    "        W_h = W_h + alpha * delta_h.dot(X.T)\n",
    "        W_y = W_y + alpha * delta_y.dot(h_hat.T)\n",
    "        \n",
    "        b_y = b_y + alpha * delta_y.sum()\n",
    "        b_h = b_h + alpha * delta_h.sum(axis=1, keepdims=True)\n",
    "\n",
    "        errors = np.append(errors, np.sum(error ** 2))\n",
    "\n",
    "    # Graficamos el error\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    visualizar_errores(t, errors, ax1)\n",
    "    visualizar_pesos(W_h, b_h, X, Y, ax2)\n",
    "    plt.show()\n",
    "\n",
    "    # Imprimimos los parámetros\n",
    "    print(\"W_h =\", W_h)\n",
    "    print(\"b_h =\", b_h)\n",
    "    print(\"W_y =\", W_y)\n",
    "    print(\"b_y =\", b_y)\n",
    "\n",
    "    return lambda X: np.where(y(y(X, W_h, b_h), W_y, b_y) < threshold, 0, 1)\n",
    "\n",
    "# Entrenamos al perceptrón para la funcion OR, tm\n",
    "@widgets.interact(nodos_capa_oculta=(1, 8), alpha=(0, 2, 0.1), tmax=(1, 1000))\n",
    "def simulate(nodos_capa_oculta=2, alpha=0.5, tmax=1000):\n",
    "    X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "    Y = np.array([[0, 1, 1, 0]])\n",
    "    perceptron_multicapa(X, Y, nodos_capa_oculta, alpha, tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda56789-dd4b-49e7-98a3-db2a353f7011",
   "metadata": {},
   "source": [
    "Ejercitamos la red aprendida con un conjunto de entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97256aff-a5ae-4e89-8c9e-b8d39a6c159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos al perceptrón para la funcion XOR\n",
    "X = np.array([[0, 0, 1, 1], \n",
    "              [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "f = perceptron_multicapa(X, Y, alpha=0.5, tmax=1000)\n",
    "y_hat = f(np.array([[1], [0]]))\n",
    "print(f\"Salida: {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6607cf90-4652-4ac4-8e5f-c0c6554ae65a",
   "metadata": {},
   "source": [
    "Se ha demostrado que las redes neuronales *feedforward* multicapa con un número suficiente de unidades ocultas entre las unidades de entrada y salida tienen una propiedad de aproximación universal: \"pueden aproximar prácticamente cualquier función de interés con el grado de precisión deseado\" (Hornik et al. 1989)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f80eec-9b97-49f0-9de5-6b65603ba986",
   "metadata": {},
   "source": [
    "## El dataset MNIST\n",
    "\n",
    "Hasta ahora trabajamos con ejemplos muy simples de compuertas lógicas. Para dar un paso más, vamos a aplicar perceptrones al conjunto de datos **MNIST** (*Modified National Institute of Standards and Technology database*), un clásico en el área de aprendizaje automático.  \n",
    "\n",
    "MNIST contiene **70.000 imágenes en escala de grises de dígitos manuscritos (0–9)**, cada una de **28×28 píxeles**. Es un benchmark muy usado porque es suficientemente grande y variado para poner a prueba modelos de clasificación, pero al mismo tiempo es manejable en una computadora personal.  \n",
    "\n",
    "En nuestro caso vamos a:  \n",
    "\n",
    "- Descargar el dataset directamente con `fetch_openml` de `sklearn.datasets`.  \n",
    "- Normalizar los valores de los píxeles al rango $[0,1]$.  \n",
    "- Codificar las etiquetas de salida en formato **one-hot**: cada dígito (0–9) se representa como un vector de 10 posiciones con un único 1 en la posición correspondiente.  \n",
    "\n",
    "De esta manera, $X$ contendrá todas las imágenes como vectores de 784 características (28×28), y $Y$ contendrá los vectores one-hot con las etiquetas de los dígitos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778db5e4-ece0-4e5f-969f-7d0675f92edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "digits = np.eye(10)\n",
    "X = np.array(mnist.data / 255.0)\n",
    "Y = np.array([digits[y] for y in mnist.target.astype(int)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638b8a6-8bb5-4d4d-8fc3-f3332ed461c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897dd77-2de6-4a45-a6cf-d380c8dd6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763599b4-a433-4dd2-8917-ed8a73f343e3",
   "metadata": {},
   "source": [
    "### Visualización de los dígitos\n",
    "\n",
    "Antes de entrenar una red neuronal, conviene explorar el dataset para hacernos una idea de cómo son las imágenes y sus etiquetas.  \n",
    "Recordemos que cada ejemplo en `X` es un vector de 784 posiciones (los píxeles de una imagen 28×28 puestos en fila), y que la etiqueta correspondiente en `Y` está codificada en formato *one-hot*.  \n",
    "\n",
    "Para facilitar la inspección, vamos a usar un widget interactivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a30ce7-3195-44f8-b68d-d8f1d11830f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(i=(0, len(X)-1))\n",
    "def visualizar_digito(i=0):\n",
    "  plt.matshow(X[i].reshape(28, 28), cmap='gray')\n",
    "  plt.show()\n",
    "  print(\"y =\", Y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc4851-fa7d-4839-9db0-f35eaceef0bb",
   "metadata": {},
   "source": [
    "Ejecutá la siguiente celda para definir el perceptrón multicapa. Es código es igual al anterior, con la novedad de que el entrenamiento lo haremos por lotes (_batches_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328c74b-d97c-4384-836f-36b7218e0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(y):\n",
    "  return sp.special.expit(y)\n",
    "\n",
    "def dg(a):\n",
    "  return a * (1.0 - a)\n",
    "\n",
    "# Definimos la función que calcula el error\n",
    "def loss(x, x_pred):\n",
    "  return x - x_pred\n",
    "\n",
    "def z(X, W, b):\n",
    "  return X @ W + b\n",
    "\n",
    "def perceptron_multicapa(X, Y, nodos_capa_oculta=128, tmax=50, alpha=0.01, batch_size=128):\n",
    "  rng = np.random.default_rng(1)\n",
    "  \n",
    "  N, d = X.shape\n",
    "  k = Y.shape[1]\n",
    "  H = nodos_capa_oculta\n",
    "\n",
    "  # Pesos de la capa oculta\n",
    "  W_h = rng.standard_normal((d, H), dtype=np.float32)\n",
    "\n",
    "  # Pesos de la capa de salida\n",
    "  W_y = rng.standard_normal((H, k), dtype=np.float32)\n",
    "  b_y = np.zeros(k, dtype=np.float32)\n",
    "  \n",
    "  # Sesgos de la capa oculta\n",
    "  b_h = np.zeros(H, dtype=np.float32)\n",
    "\n",
    "  losses = []\n",
    "  num_batches = (N + batch_size - 1) // batch_size\n",
    "\n",
    "  for t in range(tmax):\n",
    "    idx = rng.permutation(N)\n",
    "    X_shuf, Y_shuf = X[idx], Y[idx]\n",
    "\n",
    "    for b in range(num_batches):\n",
    "      s, e = b*batch_size, min((b+1)*batch_size, N)\n",
    "      Xb = X_shuf[s:e]    # (B, d)\n",
    "      Yb = Y_shuf[s:e]    # (B, k)\n",
    "\n",
    "      h_hat = g(z(Xb, W_h, b_h))\n",
    "      y_hat = g(z(h_hat, W_y, b_y))\n",
    "      \n",
    "      error = loss(Yb, y_hat)\n",
    "\n",
    "      delta_y = error * dg(y_hat)\n",
    "      delta_h = delta_y.dot(W_y.T) * dg(h_hat)\n",
    "      \n",
    "      W_h = W_h + alpha * Xb.T.dot(delta_h)\n",
    "      W_y = W_y + alpha * h_hat.T.dot(delta_y)\n",
    "      \n",
    "      b_y = b_y + alpha * delta_y.sum(axis=0)\n",
    "      b_h = b_h + alpha * delta_h.sum(axis=0)\n",
    "\n",
    "    # Validamos sobre todo el conjunto\n",
    "    h_hat = g(z(X, W_h, b_h))\n",
    "    y_hat = g(z(h_hat, W_y, b_y))\n",
    "    error = loss(Y, y_hat)\n",
    "    mse = np.mean(error ** 2)\n",
    "    losses.append(mse)\n",
    "\n",
    "    if (t+1) % 5 == 0:\n",
    "      print(f\"Época {t+1}/{tmax} - Loss: {mse:.4f}\")\n",
    "\n",
    "  # Curva de error\n",
    "  plt.figure(figsize=(6,3))\n",
    "  plt.plot(losses)\n",
    "  plt.xlabel(\"Época\")\n",
    "  plt.ylabel(\"MSE\")\n",
    "  plt.title(\"Entrenamiento\")\n",
    "  plt.show()\n",
    "\n",
    "  return W_h, b_h, W_y, b_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aceda51-d19e-44a1-8be7-8008344afb02",
   "metadata": {},
   "source": [
    "Ejecutá la celda siguiente para entrenarlo con el conjunto MNIST. Sé paciente, lleva tiempo ya que el entrenamiento se realiza en la CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af23a11-38b3-4fb9-8178-d3d70adbf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_h, b_h, W_r, b_r = perceptron_multicapa(X, Y, nodos_capa_oculta=256, tmax=25, alpha=0.01, batch_size=256)\n",
    "\n",
    "@widgets.interact(i=(0, len(X)-1))\n",
    "def inferencia(i):\n",
    "  Xi = X[i]\n",
    "  Yi = Y[i]\n",
    "  \n",
    "  plt.matshow(Xi.reshape(28, 28), cmap='gray')\n",
    "  plt.show()\n",
    "  \n",
    "  h_hat = g(z(Xi, W_h, b_h))\n",
    "  y_hat = g(z(h_hat, W_r, b_r))\n",
    "  \n",
    "  print(\"Salida inferida:\", digits[np.argmax(y_hat)])\n",
    "  print(\"Salida esperada:\", Yi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
