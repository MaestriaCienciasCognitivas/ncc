{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVs8Wx53yDAm"
   },
   "source": [
    "# Práctico 3: Modelos de memorias matriciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este práctico, trataremos de hacer el salto conceptual de una neurona a redes de neuronas. Para ello, abordaremos principalmente el capítulo 3 de *Fundamentals of computational neuroscience* de Trappenberg.\n",
    "\n",
    "*Nota: En el Cuaderno 2 vimos una introducción a Numpy y Matplotlib. En este práctico haremos una introducción aún mas breve, por lo que en caso de dudas, pueden fijarse en el Cuaderno 2.*\n",
    "\n",
    "Comenzaremos, como siempre, importando las librerías que vamos a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Para evitar imprimir números decimales muy largos, modificamos el formateador de Numpy:\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "# Importamos SciPy\n",
    "import scipy as sp\n",
    "\n",
    "# Importamos Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA1hlamNyDAv"
   },
   "source": [
    "Consideremos primero un vector $\\pmb{a}$ definido como:\n",
    "\n",
    "$\\pmb{a} = \\begin{pmatrix} a_{1} \\\\ a_{2} \\\\ a_{3} \\\\ a_{4} \\end{pmatrix}$\n",
    "\n",
    "Decimos que este vector tiene 4 componentes $a_1$, $a_2$, $a_3$ y $a_4$. Para trabajar con números en lugar de componentes abstractos, en esta breve introducción, vamos a comenzar definiendo cuatro vectores de entrada $\\pmb{e}_i$ de la siguiente forma:\n",
    "\n",
    "$\\pmb{e}_1 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{pmatrix}$\n",
    "$\\pmb{e}_2 = \\begin{pmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{pmatrix}$\n",
    "$\\pmb{e}_3 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ -0.5 \\end{pmatrix}$\n",
    "$\\pmb{e}_4 = \\begin{pmatrix} 0.5 \\\\ -0.5 \\\\ -0.5 \\\\ 0.5 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Python, y específicamente con Numpy, preferimos ver a los vectores como un caso especial de las matrices. Las matrices son colecciones de escalares o vectores. Hablamos de una matriz de $n \\times m$ donde $n$ es el número de filas y $m$ el número de columnas, y tienen la forma:\n",
    "\n",
    "$\\pmb{a} = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32} \\\\ a_{41} & a_{42} \\end{pmatrix}$\n",
    "\n",
    "Por lo tanto, los vectores son sencillamente matrices de $n \\times 1$ y se pueden definir, como es usual en Numpy, usando el método `np.array()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1663338657797,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "j22ysRQlyDAw"
   },
   "outputs": [],
   "source": [
    "e_1 = 0.5 * np.array([[1], [1], [1], [1]])\n",
    "e_2 = 0.5 * np.array([[1], [-1], [1], [-1]])\n",
    "e_3 = 0.5 * np.array([[1], [1], [-1], [-1]])\n",
    "e_4 = 0.5 * np.array([[1], [-1], [-1], [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimamos el vector $\\pmb{e_1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1663338663066,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "yLPoNzo1AkWF",
    "outputId": "c27b8917-7960-483f-e84a-a072ae618790"
   },
   "outputs": [],
   "source": [
    "print(e_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratar a los vectores como matrices tiene la ventaja de que podemos usar las mismas operaciones ya definidas para matrices de forma natural. Veamos algunas de ellas que nos serán de mucha utilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejercicio: Pruebe imprimir los demas vectores de entrada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traspuesta\n",
    "\n",
    "La *traspuesta* de una matriz se logra girándola 90 grados; la primera fila se vuelve la primera columna, la segunda fila se vuelve la segunda columna, y así hasta convertir todas las filas en columnas. Por ejemplo, la traspuesta de $\\pmb{a}$ se define como:\n",
    "\n",
    "$\\pmb{a'} = \\begin{pmatrix} a_{11} & a_{21} & a_{31} & a_{41} \\\\ a_{12} & a_{22} & a_{32} & a_{42} \\end{pmatrix}$\n",
    "\n",
    "En Python usamos la propiedad `.T` para obtener la traspuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1663338670589,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "kBBahM3m__1Z",
    "outputId": "f7a2fb13-300a-447e-8549-e42b7ee3ee7f"
   },
   "outputs": [],
   "source": [
    "print(e_1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejercicio: Pruebe imprimir la traspuesta de los demas vectores de entrada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producto escalar\n",
    "\n",
    "El producto escalar, o producto interno, es la multiplicación entre un vector columna y un vector fila. Como nuestros vectores tienen una columna, para obtener el producto escalar necesitamos trasponer uno de ellos:\n",
    "\n",
    "$\\pmb{a} \\cdot \\pmb{b} = \\pmb{a'} \\pmb{b} = \\sum_{i}a_ib_i$\n",
    "\n",
    "Usando como ejemplo los vectores $e_1$ y $e_2$:\n",
    "\n",
    "$\\begin{split}\n",
    "\\pmb{e_1'} \\pmb{e}_2 &= \\begin{pmatrix} 0.5 & 0.5 & 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{pmatrix} \\\\\n",
    "&= 0.5 \\times 0.5 + 0.5 \\times -0.5 + 0.5 \\times 0.5 + 0.5 \\times -0.5 \\\\\n",
    "&= 0.25 - 0.25 + 0.25 - 0.25 \\\\\n",
    "&= 0\n",
    "\\end{split}$\n",
    "\n",
    "En Numpy, usamos el método `.dot()` en conjunto con la traspuesta `.T` para calcular el producto escalar de dos vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1663321652427,
     "user": {
      "displayName": "Neurociencias Cognitivas Computacionales",
      "userId": "07405778412549737291"
     },
     "user_tz": 180
    },
    "id": "0BzA8SntyDAx",
    "outputId": "94d7bfbe-1d24-4d9d-f71f-9eff0993cb2c"
   },
   "outputs": [],
   "source": [
    "print(e_1.T.dot(e_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejercicio: calcule el producto escalar entre todos los vectores e, incluyendo consigo mismos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Qué propiedad cumplen los vectores $e_{i}$ para que sus productos internos den los resultados de arriba?\n",
    "\n",
    "[Hacé click para la solución](https://raw.githubusercontent.com/MaestriaCienciasCognitivas/ncc/main/book/solutions/Practico3_Pregunta.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norma\n",
    "\n",
    "El largo, o la norma $\\lVert \\pmb{a} \\rVert$, de un vector $\\pmb{a}$ se calcula como:\n",
    "\n",
    "$\\lVert \\pmb{a} \\rVert = \\sqrt{\\sum_i a_i ^ 2}$\n",
    "\n",
    "Usando el producto escalar como herramienta de sintetización, podemos reescribir la norma como:\n",
    "\n",
    "$\\lVert \\pmb{a} \\rVert = \\sqrt{\\pmb{a'} \\pmb{a}}$\n",
    "\n",
    "Vamos como hacerlo en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.sqrt(e_1.T.dot(e_1))\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejercicio: calcule la norma de todos los vectores e.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra primera red neuronal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lStOtFDmyDAy"
   },
   "source": [
    "Ahora, en una red neuronal, los vectores de entradas van a ser un vector con números que representarán las diferentes entradas que ingresan a la neurona. Se llama vector porque podría estar en un espacio n-dimensional y esto marca una flecha entre cero y ese punto. Tiene propiedades de los vectores y se puede definir el producto escalar. Cuando el mismo es 0, indica que el vector es ortogonal. Cada número de esos vectores representa la actividad de una fibra. El problema de aprendizaje sería que dada una entrada, se produzca una salida.\n",
    "\n",
    "Creemos ahora vectores de salida $s_i$, de largo 3, que estarán asociados con cada vector de entrada $e_i$ que definimos arriba.\n",
    "\n",
    "$s_1 = \\begin{pmatrix} 12 \\\\ 0.31 \\\\ \\pi \\end{pmatrix}$\n",
    "$s_2 = \\begin{pmatrix} -5 \\\\ 3.1 \\\\ 0.2 \\end{pmatrix}$\n",
    "$s_3 = \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\end{pmatrix}$\n",
    "$s_4 = \\begin{pmatrix} \\sqrt 2 \\\\ e \\\\ 0 \\end{pmatrix}$\n",
    "\n",
    "Que, recordemos, estaban definidos como:\n",
    "\n",
    "$\\pmb{e}_1 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{pmatrix}$\n",
    "$\\pmb{e}_2 = \\begin{pmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{pmatrix}$\n",
    "$\\pmb{e}_3 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ -0.5 \\end{pmatrix}$\n",
    "$\\pmb{e}_4 = \\begin{pmatrix} 0.5 \\\\ -0.5 \\\\ -0.5 \\\\ 0.5 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1663339683510,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "Rq55ezEGyDAy"
   },
   "outputs": [],
   "source": [
    "s_1 = np.array([[12], [0.31], [np.pi]])\n",
    "s_2 = np.array([[-5], [3.1], [0.2]])\n",
    "s_3 = np.array([[1], [-1], [-1]])\n",
    "s_4 = np.array([[np.sqrt(2)], [np.e], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXe0m0YNwFyD"
   },
   "source": [
    "Las salidas pueden tener distintas dimensionalidades que el vector de entrada. Si por ejemplo calculamos el producto escalar entre salida y entrada y no es cero, entonces las mismas están relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1663339685109,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "hA8RA3f0DIjv",
    "outputId": "7e0d47a5-a17c-4714-cd65-26d1e5b3257a"
   },
   "outputs": [],
   "source": [
    "print(s_1.T.dot(s_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1hNECXGyDAz"
   },
   "source": [
    "El objetivo de nuestra red neuronal es encontrar una matriz $\\pmb{m}$ de tal forma que cada vez que la multiplico por $\\pmb{e_i}$ me de como resultado el vector $\\pmb{s_i}$ correspondiente. Es decir, que cumpla:\n",
    "\n",
    "$\\begin{split}\n",
    "\\pmb{e_1} \\cdot \\pmb{m} &= \\pmb{s_1} \\\\\n",
    "\\pmb{e_2} \\cdot \\pmb{m} &= \\pmb{s_2} \\\\\n",
    "\\pmb{e_3} \\cdot \\pmb{m} &= \\pmb{s_3} \\\\\n",
    "\\pmb{e_4} \\cdot \\pmb{m} &= \\pmb{s_4} \\\\\n",
    "\\end{split}$\n",
    "\n",
    "Para encontrarlo, basta con definir a $\\pmb{m}$ como :\n",
    "\n",
    "$\\pmb{m} = \\pmb{s_1} \\cdot \\pmb{e_1} + \\pmb{s_2} \\cdot \\pmb{e_2} + \\pmb{s_3} \\cdot \\pmb{e_3} + \\pmb{s_4} \\cdot \\pmb{e_4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SziUJGJFwdMm"
   },
   "source": [
    "La versión de la regla Hebbiana es hacer el producto externo. Lo que vimos en la clase de multiplicar las entradas por las salidas y que nos genere los pesos, esa es la matriz que se va a generar en el producto externo que da como resultado una matriz. Veamos como hacerlo en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 598,
     "status": "ok",
     "timestamp": 1663339847690,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "_yoABrWlyDAz",
    "outputId": "8ce8b94a-7478-47a4-a95b-c0cec45b7893"
   },
   "outputs": [],
   "source": [
    "# Cada vez que tengo una entrada y salida, estoy multiplicando y sumando las entradas\n",
    "m = s_1.dot(e_1.T) + s_2.dot(e_2.T) + s_3.dot(e_3.T) + s_4.dot(e_4.T)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpxPL2H6yDAz"
   },
   "source": [
    "¡Felicidades! Acaba de crear su primera red neuronal, que puede asociar las entradas $\\pmb{e_i}$ con las salidas $\\pmb{s_i}$.\n",
    "\n",
    "A continuación ponemos a prueba la red. Primero verificamos que la salida del producto interno entre $\\pmb{m}$ y el vector $\\pmb{e_2}$ de como resultado el vector deseado $\\pmb{s_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 568,
     "status": "ok",
     "timestamp": 1663340108808,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "orqFiF8UyDA0",
    "outputId": "7ab682e7-fc31-4d66-b0ee-322b04801c6e"
   },
   "outputs": [],
   "source": [
    "# Imprimo la entrada e1\n",
    "print(e_1)\n",
    "\n",
    "# Imprimo la red neuronal\n",
    "print(m)\n",
    "\n",
    "# Imprimo el producto de M y e1\n",
    "print(m.dot(e_1))\n",
    "\n",
    " # Imprimo la salida esperada s1\n",
    "print(s_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejercicio: Pruebe que salida produce para las demás entradas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43kcFzMWyDA0"
   },
   "source": [
    "## Similitud de vectores\n",
    "\n",
    "Cuando comparamos vectores, muchas veces lo que nos interesa es el ángulo entre los mismos, que nos da una relación de similitud que descuenta el largo de los vectores, y que tiene varias ventajas analíticas.\n",
    "\n",
    "El ángulo entre dos vectores $\\pmb{a}$ y $\\pmb{b}$ se calcula tomando el producto interno entre los vectores, dividido por el producto de la norma de cada vector:\n",
    "\n",
    "$$\\cos{(\\theta(\\pmb{a},\\pmb{b}))} = \\frac{\\pmb{a} \\cdot \\pmb{b}}{\\lvert \\pmb{a} \\rvert \\lvert \\pmb{b} \\rvert} $$\n",
    "\n",
    "Abajo calculamos el coseno del ángulo entre la salida obtenida del producto de *e4* y *M*, y la salida esperada *s4*.\n",
    "\n",
    "**Nota:** Cuando el coseno del ángulo es 1, los vectores son paralelos, y cuando es 0 son ortogonales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1663340113163,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "ieblcByfyDA0",
    "outputId": "b07cd2f8-3b7c-4a89-fbe5-d59dd7f2f12b"
   },
   "outputs": [],
   "source": [
    "# Obtenemos la salida con la matriz m\n",
    "s_4r = m.dot(e_4)\n",
    "\n",
    "# Norma del vector obtenido con la red\n",
    "norma_s_4r = np.sqrt(s_4r.T.dot(s_4r))\n",
    "\n",
    "# Norma del vector esperado\n",
    "norma_s_4 = np.sqrt(s_4.T.dot(s_4))\n",
    "\n",
    "# Calculamos el coseno\n",
    "coseno_vectores = s_4r.T.dot(s_4) / (norma_s_4r * norma_s_4)\n",
    "\n",
    "#Imprimimos el resultado\n",
    "print(coseno_vectores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQU3hmkUyDA1"
   },
   "source": [
    "Un coseno de 1 significa que el vector que predijo la red neuronal es paralelo (equivalente) al esperado.\n",
    "\n",
    "La red neuronal $\\pmb{m}$ puede asociar correctamente las entradas $\\pmb{e_i}$ con las salidas $\\pmb{s_i}$. Pero que ocurre si en lugar de utilizar una entrada conocida, como por ejemplo $\\pmb{e_4}$, utilizamos como entrada un vector similar pero no igual a este. ¿La matriz la asocia a una salida similar a $\\pmb{s_4}$?\n",
    "\n",
    "Para averiguarlo, definimos un nuevo vector $\\pmb{e_{4p}}$ ($\\pmb{e_4}$ perturbado), sumándole 0.1 a cada entrada de $\\pmb{e_4}$:\n",
    "\n",
    "$\\pmb{e_{4p}} = \\begin{pmatrix} 0.5 + 0.1 \\\\ -0.5 + 0.1 \\\\ -0.5 + 0.1 \\\\0.5+0.1 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1663340118052,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "v2_753y5yDA1",
    "outputId": "b8764e22-2739-438f-97ab-a9accaf4d183"
   },
   "outputs": [],
   "source": [
    "# Definimos el nuevo vector de entrada\n",
    "e_4p = e_4 + 0.1\n",
    "\n",
    "# Obtenemos la salida con la matriz m para ambas entradas\n",
    "s_4r = m.dot(e_4)\n",
    "s_4p = m.dot(e_4p)\n",
    "\n",
    "# Imprimimos la salida de la red con el vector original\n",
    "print(s_4r)\n",
    "\n",
    "# Imprimimos la salida de la red con nuevo vector perturbado\n",
    "print(s_4p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Son estas salidas similares? Usemos el coseno como herramienta para medirlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norma del vector obtenido\n",
    "norma_s_4p = np.sqrt(s_4p.T.dot(s_4p))\n",
    "\n",
    "# Norma del vector esperado\n",
    "norma_s_4 = np.sqrt(s_4.T.dot(s_4))\n",
    "\n",
    "# Calculamos el coseno\n",
    "coseno_vectores = s_4p.T.dot(s_4 ) / (norma_s_4p * norma_s_4)\n",
    "\n",
    "#Imprimimos el resultado\n",
    "print(coseno_vectores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un coseno cercano a 1 significa que la red neuronal no solo puede asociar correctamente los vectores de entrada a los de salida esperados, sino que también puede asociar vectores parecidos a los de entrada a vectores parecidos a los de salida. Es decir, puede asociar vectores nuevos, no solo los que aprendió."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9gx5CJcyDA1"
   },
   "source": [
    "## Red neuronal 2.0\n",
    "\n",
    "A continuación hacemos una nueva red un poco más grande. Para eso primero vamos a crear nuestras entradas 'ortonormales', que la red puede recordar fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 652,
     "status": "ok",
     "timestamp": 1663340282551,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "d2Pf1HfZyDA1",
    "outputId": "7d0a8e11-c304-4e28-d7e4-7ab96c7e091b"
   },
   "outputs": [],
   "source": [
    "# Primero generamos una matriz de números aleatorios entre 0 y 1, con un tamaño de 100 filas y 30 columnas\n",
    "A = np.random.random((100,30))\n",
    "print(f\"La matriz A tiene forma\", A.shape)\n",
    "print(f\"Esta es la primera columna de A\\n{A[:,1]}\")\n",
    "print(f\"Este es el producto interno de las columnas 1 y 2: {(A[:,1].T).dot(A[:,2]):.1f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1663340289909,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "USTaQgwayDA2",
    "outputId": "89536474-10f6-4332-cb06-c15b691caaa7"
   },
   "outputs": [],
   "source": [
    "# Luego usamos un truco de algebra lineal (llamado SVD) que nos permite obtener\n",
    "# una matriz con 30 columnas ortogonales a partir de A\n",
    "E = sp.linalg.orth(A)\n",
    "print(f\"La matriz E tiene forma\", E.shape)\n",
    "print(f\"Esta es la primera columna de E\\n{E[:,1]}\")\n",
    "\n",
    "# Vemos que el producto interno entre columnas sea 0 (o cercano). Pruebe otras columnas\n",
    "print(f\"Este es el producto interno de las columnas 1 y 2: {(E[:,1].T).dot(E[:,2]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKsnFtLbzfXS"
   },
   "source": [
    "La matriz E tiene columnas que son ortogonales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1663340293917,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "TW_TqQF2JRFN",
    "outputId": "a635231e-9338-4b3f-a243-0e56fffc7818"
   },
   "outputs": [],
   "source": [
    "print(f\"{(E[:,2].T).dot(E[:,4]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mvROzoNyDA2"
   },
   "source": [
    "**Preguntas:** ¿Cuál es la norma de cada columna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giRHmDVkyDA2"
   },
   "outputs": [],
   "source": [
    "norma = np.sqrt(E[:,7].T.dot(E[:,7]))\n",
    "print(norma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3ZXJ9S2yDA2"
   },
   "source": [
    "Ahora creamos nuestras salidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1225,
     "status": "ok",
     "timestamp": 1663340314219,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "JjwKFvRVyDA3"
   },
   "outputs": [],
   "source": [
    "S = np.eye(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejercicio: Imprima las salidas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQLbYqBmyDA3"
   },
   "source": [
    "Y creemos la red neuronal de la misma forma que lo hicimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1663340322062,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "Xfku54tdyDA3"
   },
   "outputs": [],
   "source": [
    "M = S.dot(E.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v0whngOyDA4"
   },
   "source": [
    "Probemos como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 973,
     "status": "ok",
     "timestamp": 1663340379524,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "I9qEwH0DyDA4",
    "outputId": "feb7ae86-f733-4a3b-a6f4-23a779996dbc"
   },
   "outputs": [],
   "source": [
    "#E 2 es la tercera columna de la matriz de entrada. El tercer lugar da un uno, que se explica por la salida correspondiente.\n",
    "# Cada una de las columnas de E, se asocia a las salidas de S.\n",
    "salida = M.dot(E[:,3])\n",
    "print(salida*(abs(salida)>0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiLOML0syDA5"
   },
   "source": [
    "Es bueno mirar como queda la matriz M. Para eso hacemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1663340524441,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "X69bQHMoyDA6",
    "outputId": "7c9fdfd7-6a8e-4da6-871b-40d864a0ca26"
   },
   "outputs": [],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos mirar como queda la matriz M utilizando Matplotlib para dibujar las filas y columnas con colores, un color oscuro significa que hay un valor cercano a 0, mientras que un color claro significa que es un valor cercano a 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(M, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdoEYrVJyDA6"
   },
   "source": [
    "Vamos a intentar hacer lo siguiente. vamos destruyendo la matriz al azar probamos qué tanto correlacionan las salidas deseadas con las salidas obtenidas. Primero a mano y viendo el resultado. Luego haremos un programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 646,
     "status": "ok",
     "timestamp": 1663341083829,
     "user": {
      "displayName": "Richard Rodríguez",
      "userId": "10069262491529789019"
     },
     "user_tz": 180
    },
    "id": "nvyG4AFGyDA6",
    "outputId": "b9dc91e2-d1e2-4695-c51a-34ae977539df"
   },
   "outputs": [],
   "source": [
    "# Copio la red para no modificarla\n",
    "Mp = M.copy()\n",
    "\n",
    "# Defino una entrada cualquiera\n",
    "E_i = E[:,2]\n",
    "\n",
    "# Pruebo una entrada\n",
    "S_r = Mp.dot(E_i)\n",
    "\n",
    "# Pongo una componente cualquiera de la matriz igual a 0\n",
    "Mp[7,:] = 0 \n",
    "\n",
    "# Pruebo de vuelta\n",
    "S_rp = Mp.dot(E_i)\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(f\"Salida esperada: {S_r}\")\n",
    "print(f\"Salida obtenida: {S_rp}\")\n",
    "\n",
    "# Calculamos sus normas\n",
    "norma_S_r = np.sqrt(S_r.T.dot(S_r))\n",
    "norma_S_rp = np.sqrt(S_rp.T.dot(S_rp))\n",
    "\n",
    "# Calculamos el coseno\n",
    "coseno_vectores = S_r.T.dot(S_rp) / (norma_S_r * norma_S_rp)\n",
    "\n",
    "# Imprimimos el coseno\n",
    "print(f\"Similitud: {coseno_vectores:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "\n",
    "1. Si yo destruyo pesos de la matriz la misma sigue funcionando, el lugar correspondiente dará un valor bien cercano a 1.\n",
    "2. Tiene resistencia a la rotura porque hay mucha redundancia en la matriz. Esto nos permite ir rompiendo y acercándonos a 1.\n",
    "3. Vemos como el coseno del angulo entre el vector salida y el vector entrada siempre está cerca de 1.\n",
    "4. Esto es una propiedad interesante vinculada a las memorias de corte biológico. Se puede romper o perturbar el vector y el sistema da una salida medianamente correcta.\n",
    "5. La tolerancia al daño y la capacidad para generalizar son la base de los fenómenos colectivos de estas redes neuronales. La propiedad distribuida por varios lugares. La memoria está en los pesos.\n",
    "6. En este modelo de memoria, yo no represento a los objetos, tengo información para reconstruir la representación de los objetos.\n",
    "7. Almacenamiento de la información superpuesta y distribuida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9beBD4YyDA6"
   },
   "source": [
    "Podemos hacer una evaluación más sistemática. Por ejemplo, podemos romper en forma sistemática la red neuronal (en lugar de hacerlo a mano) de tal forma que podamos romper un porcentaje dado.\n",
    "\n",
    "Antes de pasar al código, veamos primero como podemos romper una matriz. Supongamos que tenemos definido una matriz $\\pmb{a}$ de la siguiente forma:\n",
    "\n",
    "$\\pmb{a} = \\begin{pmatrix} a_{11} & a_{21} & a_{31} & a_{41} \\\\ a_{12} & a_{22} & a_{32} & a_{42} \\end{pmatrix}$\n",
    "\n",
    "Si construimos una matriz de ceros y unos, por ejemplo,\n",
    "\n",
    "$\\pmb{b} = \\begin{pmatrix} 0 & 1 & 1 & 0 \\\\ 1 & 1 & 0 & 0 \\end{pmatrix}$\n",
    "\n",
    "Podemos hacer una multiplicación componente a componente. Eso en matemática NO ES el producto común de las matrices; se lo conoce como producto de Hadamard y a veces se anota con un círculo con un punto inscripto $\\odot$. En Python multiplicar dos matrices con * da por defecto ese producto. Al multiplicar la matriz $\\pmb {a}$ por la $\\pmb {b}$ obtenemos:\n",
    "\n",
    "$\\pmb{a} * \\pmb{b} = \\begin{pmatrix} 0 & a_{21} & a_{31} & 0 \\\\ a_{12} & a_{22} & 0 & 0 \\end{pmatrix}$\n",
    "\n",
    "La clave estará entonces en controlar la cantidad de ceros y unos que tiene esta matriz $\\pmb{b}$. En Python, para obtener una matriz con un 80% de ceros, podemos hacerlo de la siguiente forma:\n",
    "\n",
    "```\n",
    "b = np.random.choice([0, 1], size=M.shape, p=[0.8, 1-0.8])\n",
    "```\n",
    "\n",
    "Veámosla en accion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(r=(0, 1, 0.1))\n",
    "def simulate(r=0.8):\n",
    "    a = M.copy()\n",
    "    b = np.random.choice([0, 1], size=a.shape, p=[r, 1-r])\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, layout=\"constrained\", sharex=True)\n",
    "    ax1.matshow(a, cmap=\"coolwarm\", vmin=M.min(), vmax=M.max())\n",
    "    ax2.matshow(b, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax3.matshow(a*b, cmap=\"coolwarm\", vmin=M.min(), vmax=M.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, podemos proceder a perturbar la red neuronal 2.0 en forma sistemática, y medir que tan bien sigue precidiendo los vectores de salida esperados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1663321975326,
     "user": {
      "displayName": "Neurociencias Cognitivas Computacionales",
      "userId": "07405778412549737291"
     },
     "user_tz": 180
    },
    "id": "FlXG8o9myDA6",
    "outputId": "4afddea2-326e-4192-e9d3-cf4f6afd6363"
   },
   "outputs": [],
   "source": [
    "# Normaliza un vector\n",
    "# Da como resultado un vector de norma 1\n",
    "def normalizar(v):\n",
    "    norma = np.sqrt(v.T.dot(v))\n",
    "    if norma == 0:\n",
    "        return v\n",
    "    else:\n",
    "        return v / norma\n",
    "\n",
    "# Dada una red neuronal Mp, una matriz de entrada E y una matriz de salida esperada S:\n",
    "# Para cada par de vectores de entrada E_i y de salida S_i, calcula el resultado de la red neuronal\n",
    "# Y devuelve el promedio de todos los cosenos.\n",
    "def evaluanet(Mp, E, S):\n",
    "    # Obtenemos el numero de filas y columnas\n",
    "    m = E.shape[0]\n",
    "    n = E.shape[1]\n",
    "\n",
    "    # Inicializamos un vector que va a guardar los cosenos para cada par de vectores de salida esperada\n",
    "    # y salida obtenida\n",
    "    cosenos = 0\n",
    "\n",
    "    # Recorremos cada columna\n",
    "    for i in np.arange(n):\n",
    "        # Creamos los vectores de entrada y salida esperada\n",
    "        E_i = E[:,i]\n",
    "        S_i = S[:,i]\n",
    "\n",
    "        # Creamos el vector resultado de la red neural\n",
    "        S_ir = Mp.dot(E_i)\n",
    "\n",
    "        # Calculamos la similitud entre la salida esperada y el resultado obtenido con la red\n",
    "        cosenos += normalizar(S_i).T.dot(normalizar(S_ir))\n",
    "\n",
    "    # Devolvemos el promedio\n",
    "    return cosenos / n\n",
    "\n",
    "# Creamos un vector de largo 20 con valores entre 0 y 1\n",
    "# Por ejemplo, si fuese de largo 3, quedaria: [0, 0.5, 1]\n",
    "destroy_rates = np.linspace(0, 1, 20, endpoint=True)\n",
    "\n",
    "# Creamos un vector \n",
    "correlaciones = np.zeros(len(destroy_rates))\n",
    "\n",
    "j = 0\n",
    "for rd in destroy_rates:\n",
    "    # Creamos una matriz de misma forma que M con ceros y unos\n",
    "    # Con un porcentaje de ceros dado por rd\n",
    "    destroy_matrix = np.random.choice([0, 1], size=M.shape, p=[rd, 1-rd])\n",
    "\n",
    "    # Perturbamos M\n",
    "    Mp = M * destroy_matrix\n",
    "\n",
    "    # Vemos que tan bien predice la red neuronal\n",
    "    correlaciones[j] = evaluanet(Mp, E, S)\n",
    "    j=j+1\n",
    "\n",
    "# Graficamos las correlaciones\n",
    "plt.plot(destroy_rates, correlaciones)\n",
    "plt.xlabel(\"r\")\n",
    "plt.ylabel(\"Similitud promedio\")\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
