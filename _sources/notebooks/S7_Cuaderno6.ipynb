{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZdKPsBaCMPb"
   },
   "source": [
    "# Cuaderno 6: Convoluciones\n",
    "\n",
    "En este cuaderno mostraremos algunos conceptos básicos sobre la convolución.\n",
    "\n",
    "Para comprender la visión humana, es esencial entender las primeras etapas del procesamiento de imágenes en la vía visual. Un problema común al encontrar bordes es que las imágenes no son perfectas; contienen imperfecciones conocidas como ruido. La teoría aborda el problema del ruido desenfocando las imágenes, mediante un proceso llamado convolución. Básicamente, consiste en aplicar un tipo particular de operador a lo largo de la imagen. Este operador tiene un perfil gaussiano porque la teoría computacional especifica que esta es la forma que optimiza la combinación de suavizar el ruido sin afectar demasiado las zonas donde se encuentran los bordes en la imagen convolucionada. El siguiente paso es identificar las regiones de la imagen donde hay cambios abruptos en las intensidades, ya que ahí es donde están los bordes. Esto requiere medir gradientes de intensidad y/o cambios en los gradientes, que se conocen como la primera y segunda derivadas, respectivamente. Se describen varios algoritmos biológicamente plausibles para implementar la teoría. Estos algoritmos implican operadores notablemente similares a los campos receptivos de las células en la retina y la corteza estriada.\n",
    "\n",
    "Las representaciones cerebrales de los bordes no pueden ser lo único implicado en la visión, ya que somos capaces de describir características de la escena mucho más complejas que los bordes. Aun así, las representaciones de los bordes pueden ser útiles de inmediato para guiar una acción de agarre en torno a un objeto. Además, pueden servir como un primer paso importante en tareas perceptuales más complejas, como el reconocimiento de objetos o la percepción de profundidad.\n",
    "\n",
    "- Capítulos 3, 5 y 9 de Frisby, J. P. & Stone, J. V. *Seeing*. (The MIT Press, London, 2010).\n",
    "- Capítulo 7 de Trappenberg, T. P. *Fundamentals of Computational Neuroscience*. (Oxford University Press, Oxford, 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración\n",
    "\n",
    "Comenzamos importando las librerías que vamos a usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkCJ-4E6AA-g",
    "outputId": "03113e00-f8cf-4cd7-ac96-e876de686351"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones utilitarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(\"Device encontrado:\", device)\n",
    "    return device\n",
    "    \n",
    "def train_net(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_net(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de graficado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_convolucion1d(s, f, convolucion):\n",
    "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, width_ratios=[2, 1, 2], figsize=(12, 3))\n",
    "\n",
    "  ax1.set_title(\"Señal original\")\n",
    "  ax1.plot(s)\n",
    "  ax1.set_ylim(-0.1, 1.1)\n",
    "  \n",
    "  ax2.set_title(\"Filtro\")\n",
    "  ax2.imshow(f.reshape(1, f.shape[0]), cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "  ax2.set_axis_off()\n",
    "  \n",
    "  ax3.set_title(\"Resultado de la convolución\")\n",
    "  ax3.plot(convolucion)\n",
    "  ax3.set_ylim(-1, 1)\n",
    "  \n",
    "  fig.subplots_adjust(wspace=0.4)\n",
    "  \n",
    "  pos1 = ax1.get_position()\n",
    "  pos2 = ax2.get_position()\n",
    "  pos3 = ax3.get_position()\n",
    "\n",
    "  x_plus = (pos1.x1 + pos2.x0) / 2\n",
    "  x_eq = (pos2.x1 + pos3.x0) / 2\n",
    "\n",
    "  y_center = (pos1.y0 + pos1.y1) / 2\n",
    "\n",
    "  fig.text(x_plus, y_center, '+', ha='center', va='center', fontsize=16)\n",
    "  fig.text(x_eq, y_center, '=', ha='center', va='center', fontsize=16)\n",
    "  \n",
    "  plt.show()\n",
    "    \n",
    "def visualizar_convolucion2d(s, f, convolucion):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, width_ratios=[2, 1, 2], figsize=(12, 6))\n",
    "\n",
    "    ax1.set_title(\"Imagen original\")\n",
    "    ax1.matshow(s, cmap=\"gray\")\n",
    "    ax1.set_axis_off()\n",
    "    \n",
    "    ax2.set_title(\"Filtro\")\n",
    "    ax2.imshow(f, cmap=\"coolwarm\")\n",
    "    ax2.set_axis_off()\n",
    "    \n",
    "    ax3.set_title(\"Resultado de la convolución\")\n",
    "    ax3.imshow(convolucion, cmap=\"gray\")\n",
    "    ax3.set_axis_off()\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.2)\n",
    "    fig.text(0.420, 0.5, '+', ha='center', va='center', fontsize=16)\n",
    "    fig.text(0.605, 0.5, '=', ha='center', va='center', fontsize=16)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualizar_pesos(conv_layer, in_channel=0, max_plots=64):\n",
    "  W = conv_layer.weight.detach().cpu()  # (out_ch, in_ch, kH, kW)\n",
    "  n = min(W.shape[0], max_plots)\n",
    "  cols = int(math.ceil(math.sqrt(n)))\n",
    "  rows = int(math.ceil(n / cols))\n",
    "  \n",
    "  fig, axes = plt.subplots(rows, cols, figsize=(2*cols, 2*rows))\n",
    "  axes = axes.flatten()\n",
    "  \n",
    "  for i in range(n):\n",
    "      ker = W[i, in_channel].numpy()\n",
    "      axes[i].imshow(ker, cmap=\"bwr\")  # bwr: azul negativo, rojo positivo\n",
    "      axes[i].axis(\"off\")\n",
    "  for j in range(n, len(axes)):\n",
    "      axes[j].axis(\"off\")\n",
    "  plt.suptitle(f\"Pesos {conv_layer.__class__.__name__}\")\n",
    "  plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración\n",
    "\n",
    "*Imagenette* es un subconjunto reducido del famoso dataset ImageNet, diseñado para fines educativos y de experimentación rápida en visión por computadora. Contiene 10 clases seleccionadas y un número mucho menor de imágenes que el conjunto original, lo que permite entrenar y evaluar modelos en menos tiempo sin perder el realismo de trabajar con imágenes naturales.\n",
    "\n",
    "Para comenzar, ejecutá la celda siguiente y se descargará automáticamente el conjunto de entrenamiento y de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Imagenette\n",
    "from torchvision.transforms import Compose, Grayscale, Resize, ToTensor\n",
    "\n",
    "transform = Compose([Grayscale(), Resize((224, 224)), ToTensor()])\n",
    "try:\n",
    "  train_dataset = Imagenette(root='data', split=\"train\", size=\"160px\", transform=transform, download=False)\n",
    "except RuntimeError:\n",
    "  train_dataset = Imagenette(root='data', split=\"train\", size=\"160px\", transform=transform, download=True)\n",
    "val_dataset = Imagenette(root='data', split=\"val\", size=\"160px\", transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para explorar el conjunto de validación, a continuación se muestra un *widget* interactivo que permite visualizar imágenes individuales junto con su etiqueta numérica y el nombre de la clase correspondiente. Moviendo el control deslizante se puede recorrer el dataset y observar ejemplos reales de las distintas categorías de *Imagenette*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(idx=(0, len(val_dataset)-1))\n",
    "def visualizar_imagen(idx):\n",
    "    imagen, clase = val_dataset[idx]\n",
    "    plt.imshow(imagen.squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    print(f'Etiqueta: {clase}, Nombre: {val_dataset.classes[clase]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirá la escena que tenés enfrente: seguro podés distinguir sin problema los bordes que marcan la silueta del perrito, el contraste entre su pelo y el fondo, y hasta las sombras y detalles de su superficie. Esta facilidad para ver contornos refleja que en nuestro cerebro hay representaciones pensadas para procesar justamente esas características de borde.\n",
    "\n",
    "En computación, una imagen no es más que una matriz de números donde cada valor indica la intensidad de un píxel. En la celda que sigue vas a poder ver la imagen completa del perrito junto con una región de interés marcada con un rectángulo azul. Esa región se puede mover usando el widget interactivo: simplemente desplazá los valores de x0 y y0 con los controles deslizantes para elegir qué parte de la imagen recortar y observar en detalle. A la derecha se muestra el recorte correspondiente, lo que permite explorar cómo se representa numéricamente cada sector de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import ipywidgets as widgets\n",
    "\n",
    "imagen, clase = val_dataset[600]\n",
    "imagen = imagen.squeeze()\n",
    "roi_size=20\n",
    "\n",
    "@widgets.interact(x0=(0,imagen.shape[1]-roi_size), y0=(0,imagen.shape[0]-roi_size))\n",
    "def roi(x0, y0):\n",
    "  # Definimos una región de interés\n",
    "  x1, y1 = x0 + roi_size, y0 + roi_size\n",
    "  roi = imagen[y0:y1, x0:x1]\n",
    "\n",
    "  # Graficamos ambas matrices con un rectángulo sobre la imagen original\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, layout=\"tight\")\n",
    "  \n",
    "  ax1.imshow(imagen, cmap='gray')\n",
    "  rect = patches.Rectangle((x0, y0), x1-x0, y1-y0,  linewidth=2, edgecolor='blue', facecolor='none')\n",
    "  ax1.add_patch(rect)\n",
    "  \n",
    "  ax2.imshow(roi, cmap='gray')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuK_abkPugdf"
   },
   "source": [
    "Probá con diferentes regiones de interés. Buscá una donde aparezca parte de la oreja del perrito. ¿El borde que se ve ahí es nítido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbK0ngx8LRQJ"
   },
   "source": [
    "Para tener una mejor intuición de los cambios de luminancia en la imagen, graficamos abajo los valores de luminancia a lo largo de una fila. También graficamos el cambio en los valores de luminancia, es decir, la diferencia entre pixeles consecutivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(x0=(0,imagen.shape[1]-roi_size), y=(0,imagen.shape[0]-1))\n",
    "def roi(x0, y):\n",
    "  # Definimos una región de interés\n",
    "  x1 = x0 + roi_size\n",
    "  roi = imagen[y, x0:x1]\n",
    "\n",
    "  # Graficamos ambas matrices con un rectángulo sobre la imagen original\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "  \n",
    "  ax1.imshow(imagen, cmap='gray')\n",
    "  rect = patches.Rectangle((x0, y - 0.5), roi_size, 1, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "  ax1.add_patch(rect)\n",
    "  \n",
    "  ax2.set_title(\"Cambio de luminancia\")\n",
    "  ax2.plot(roi)\n",
    "  ax2.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-tL7ZnAugdf"
   },
   "source": [
    "Probá con diferentes filas y buscá un corte donde aparezcan las orejas del perrito. ¿Es fácil distinguir sus bordes a partir de las gráficas que muestran la diferencia de luminosidad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb3L2mNBugdf"
   },
   "source": [
    "## Convoluciones en una dimensión\n",
    "\n",
    "Hasta acá exploramos la imagen del perrito y vimos cómo cambian las intensidades al mover una región de interés. Si ahora tomamos una sola fila que cruce, por ejemplo, el borde de la oreja, esas intensidades forman una señal en una dimensión: una lista de números que suben o bajan según el contraste entre el pelo y el fondo. Detectar ese borde en una dimensión se puede hacer con diferencias entre píxeles vecinos... o, más elegante y general, con una convolución usando un *kernel* chiquito. Primero lo vemos en 1D, y en un rato lo extendemos a 2D para operar sobre la imagen completa.\n",
    "\n",
    "Para hacerlo revisaremos una operación matemática importante llamada **convolución**. Una convolución es una operación matemática que combina dos funciones para producir una tercera. En el contexto de procesamiento de imágenes y redes neuronales, la convolución es usada para extraer características importantes, como bordes, texturas y patrones.\n",
    "\n",
    "Imaginá que tenés una imagen (que puedes ver como una matriz de píxeles) y un filtro o \"kernel\" (otra matriz más pequeña). La convolución consiste en deslizar este filtro sobre la imagen, multiplicando los valores de los píxeles por los valores del filtro y sumando los resultados en cada posición. Este proceso se repite a lo largo de toda la imagen, y el resultado es una nueva imagen donde se han resaltado ciertos rasgos de la imagen original, dependiendo del filtro usado.\n",
    "\n",
    "Comenzaremos viendo que la convolución en una dimension. Luego la generalizaremos a dos dimensiones. Por ahora, el objetivo es detectar los bordes en una fila usando una convolución en lugar de la diferencia de píxeles consecutivos.\n",
    "\n",
    "Supongamos que tenemos una señal $s$ dada por\n",
    "\n",
    "$$s= \\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "Cuando realizamos una convolución de una señal con un filtro pequeño, como el filtro $f=[−1,1]$, el proceso consiste en superponer el filtro sobre los primeros elementos de la señal, multiplicar los elementos correspondientes y luego sumar los resultados para obtener el primer valor de la señal filtrada.\n",
    "\n",
    "Por ejemplo, supongamos que estamos convolucionando la señal $s$ con el filtro $f$. La primera operación sería:\n",
    "\n",
    "$$(f*s)(0)=f(0)*s(0)+f(1)*s(1)=(−1)*s(0)+(1)*s(1)=0$$\n",
    "\n",
    "Luego, repetimos el cálculo desplazando el filtro una posición en la señal:\n",
    "\n",
    "$$(f*s)(1)=f(0)*s(1)+f(1)*s(2)=(−1)*s(1)+(1)*s(2)=0$$\n",
    "\n",
    "Hacemos esto para todas las posiciones posibles de la señal original. Al aplicar el filtro a toda la señal, obtenemos un nuevo vector, que en este caso es:\n",
    "\n",
    "$$(f*s)=[0,0,0,0,1,0,0,0,0]$$\n",
    "\n",
    "Este vector resultante representa la señal después de ser filtrada, donde las operaciones de convolución han resaltado un cambio en la señal en la posición central.\n",
    "\n",
    "Veamos como hacerlo en Python usando la función de SciPy [`convolve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "Teqy0ukZugdg",
    "outputId": "d79d6b9e-a2ed-443b-f502-1e61bebeb044"
   },
   "outputs": [],
   "source": [
    "s = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "f = np.array([-1, 1])\n",
    "convolucion = -sp.signal.convolve(s, f, mode='valid')\n",
    "\n",
    "print(f\"s: {s}\")\n",
    "print(f\"f: {f}\")\n",
    "print(f\"Convolución: {convolucion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mejor cómo funciona la convolución en una dimensión, vamos a usar una función auxiliar que grafica tres cosas: la señal original, el filtro y el resultado de la convolución. De esta forma podemos ver de manera intuitiva cómo el filtro resalta los cambios en la señal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizar_convolucion1d(s, f, convolucion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpGOQ4WLugdg"
   },
   "source": [
    "Podría ser interesante notar que la señal filtrada resultante, en este caso particular de filtro, está extrayendo justamente el cambio en la señal original.\n",
    "\n",
    "Apliquémosla ahora al ejemplo del perrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_size=20\n",
    "\n",
    "out_img  = widgets.Output(layout=widgets.Layout())\n",
    "out_conv = widgets.Output(layout=widgets.Layout())\n",
    "\n",
    "@widgets.interact(x0=(0,imagen.shape[1]-roi_size), y=(0,imagen.shape[0]-1))\n",
    "def roi(x0, y):\n",
    "    # Definimos una región de interés\n",
    "    x1 = x0 + roi_size\n",
    "    roi = imagen[y, x0:x1]\n",
    "    \n",
    "    # Calculamos la convolución\n",
    "    s = roi\n",
    "    f = np.array([-1, 1])\n",
    "    convolucion = sp.signal.convolve(s, np.flip(f), mode='valid')\n",
    "\n",
    "    with out_img:\n",
    "        out_img.clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "        ax.set_title(\"Región de interés\")\n",
    "        ax.imshow(imagen, cmap='gray')\n",
    "        rect = patches.Rectangle((x0, y - 0.5), roi_size, 1, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.set_axis_off()\n",
    "        plt.show()\n",
    "      \n",
    "    with out_conv:\n",
    "        out_conv.clear_output(wait=True)\n",
    "        visualizar_convolucion1d(s, f, convolucion)\n",
    "\n",
    "# Display the stacked boxes once\n",
    "display(widgets.HBox([out_img, out_conv]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2vb5Sjfugdg"
   },
   "source": [
    "¿Cómo se compara el resultado de aplicar esta convolución con el filtro $f=[-1, 1]$ respecto al cálculo directo de la diferencia de luminosidad entre dos píxeles consecutivos en la imagen del perrito? ¿Son iguales o presentan alguna diferencia?\n",
    "\n",
    "El filtro $f=[1, 4, 7, 4, 1]$ corresponde a un ejemplo de filtro gaussiano suavizante. Probá realizar, en el mismo código anterior, una convolución con este filtro. ¿Qué efecto observás en la imagen del perrito? ¿Pensás que un suavizado de este tipo podría servir para resaltar bordes, o más bien cumple otra función?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUQUgxP7IFj4"
   },
   "source": [
    "## Convoluciones en dos dimensiones\n",
    "\n",
    "Lo que vimos en 1D (tomar una fila y detectar cambios) ahora lo generalizamos a imágenes completas. En 2D, un filtro o kernel es una pequeña matriz (p. ej., $3 \\times 3$, $5 \\times 5$) que se desliza sobre la imagen: en cada posición multiplicamos elemento a elemento los píxeles del parche por los valores del kernel y sumamos. Ese valor va al píxel de salida.\n",
    "\n",
    "Según el kernel, podemos suavizar (gaussiano), realzar bordes (Sobel/Prewitt/Laplaciano) o resaltar texturas.\n",
    "\n",
    "En la celda que sigue definimos un kernel 2D y lo aplicamos a la imagen del perrito. Tips útiles:\n",
    "* Usá tamaños impares (3, 5, 7, etc) para que el kernel tenga un centro bien definido.\n",
    "* Los kernels de borde suelen sumar 0 (responden a cambios, no a regiones planas).\n",
    "* Los de suavizado conviene normalizarlos para conservar el brillo promedio.\n",
    "\n",
    "Usaremos un kernel tipo Prewitt en x (bordes verticales). Para una visualización más clara, miraremos el valor absoluto de la respuesta (o la magnitud del gradiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736
    },
    "id": "Sn_GJAIEugdh",
    "outputId": "5974d0d1-4908-42c1-884b-81cf1ef11365"
   },
   "outputs": [],
   "source": [
    "s = imagen\n",
    "f = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "convolucion = sp.signal.convolve(s, np.flip(f), mode='same')\n",
    "visualizar_convolucion2d(s, f, convolucion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probamos el mismo detector en x pero con polaridad invertida, usando el kernel\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & -1\\\\\n",
    "1 & 0 & -1\\\\\n",
    "1 & 0 & -1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Debería resaltar los mismos bordes que antes, pero con signo opuesto (oscuro a claro vs claro a oscuro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "convolucion = sp.signal.convolve(s, np.flip(f), mode='same')\n",
    "visualizar_convolucion2d(s, f, convolucion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tMzvIO9ugdh"
   },
   "source": [
    "Otros ejemplos de filtros de convolución son:\n",
    "\n",
    "$$sharpen = \\begin{bmatrix}0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & 1 & 0\\end{bmatrix}$$\n",
    "\n",
    "$$emboss = \\begin{bmatrix}-2 & -1 & 0 \\\\ -1 & 1 & 1 \\\\ 0 & 1 & 2\\end{bmatrix}$$\n",
    "\n",
    "$$outline = \\begin{bmatrix}-1 & -1 & -1 \\\\ -1 & 8 & -1 \\\\ -1 & -1 & -1\\end{bmatrix}$$\n",
    "\n",
    "$$top sobel = \\begin{bmatrix}1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1\\end{bmatrix}$$\n",
    "\n",
    "$$right sobel = \\begin{bmatrix}-1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonificación 1: Filtros de detección de contraste\n",
    "\n",
    "En el procesamiento temprano de la información visual, un mecanismo fundamental consiste en resaltar las diferencias de luminancia entre regiones vecinas de la imagen. Para ello se utilizan filtros de detección de contraste, diseñados con una organización de tipo centro-periferia: el píxel central ejerce una influencia excitatoria, mientras que el entorno inmediato contribuye de manera inhibitoria. Al equilibrar estas dos zonas de modo que los pesos totales sumen cero, el filtro se vuelve sensible únicamente a los cambios locales de intensidad, suprimiendo áreas uniformes y potenciando los bordes y transiciones que marcan la estructura de la escena. Este tipo de operación no es solo un recurso computacional: tiene un correlato biológico en la organización de los campos receptivos de las células ganglionares de la retina y de las neuronas en el núcleo geniculado lateral, que responden de manera selectiva a contrastes espaciales semejantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([\n",
    "    [1/8, 1/8, 1/8],\n",
    "    [1/8, -1, 1/8],\n",
    "    [1/8, 1/8, 1/8]\n",
    "])\n",
    "convolucion = sp.signal.convolve(s, np.flip(f), mode='same')\n",
    "visualizar_convolucion2d(s, f, convolucion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([\n",
    "    [-1/8, -1/8, -1/8],\n",
    "    [-1/8,  1,   -1/8],\n",
    "    [-1/8, -1/8, -1/8]\n",
    "])\n",
    "convolucion = sp.signal.convolve(s, np.flip(f), mode='same')\n",
    "visualizar_convolucion2d(s, f, convolucion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQjIwGxaVk3J"
   },
   "source": [
    "Anteriormente vimos el ejemplo de aplicar un filtro gaussiano en una dimensión. Ahora veremos que sucede si, antes de aplicar un filtro de detección de bordes, aplicamos un filtro gausiano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yGFsAU2-ugdh",
    "outputId": "51bc87eb-7e39-4e13-8269-3fcfc3645910"
   },
   "outputs": [],
   "source": [
    "f1 = np.array([\n",
    "    [1/16, 2/16, 1/16],\n",
    "    [2/16, 4/16, 2/16],\n",
    "    [1/16, 2/16, 1/16]\n",
    "])\n",
    "f2 = np.array([\n",
    "    [-1/8, -1/8, -1/8],\n",
    "    [-1/8, 1, -1/8],\n",
    "    [-1/8, -1/8, -1/8]\n",
    "])\n",
    "\n",
    "primera_convolucion = sp.signal.convolve(s, np.flip(f1))\n",
    "segunda_convolucion = sp.signal.convolve(primera_convolucion, np.flip(f2))\n",
    "\n",
    "visualizar_convolucion2d(s, f1, primera_convolucion)\n",
    "visualizar_convolucion2d(primera_convolucion, f2, segunda_convolucion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cebMPARcV5rX"
   },
   "source": [
    "¿Mejora la detección de bordes? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonificación 2: nuestra segunda red convolucional\n",
    "\n",
    "En esta sección volvemos a las redes neuronales convolucionales (CNN) para conectarlas con lo que ya hicimos (diferencias, convoluciones 1D/2D y detección de bordes). Las CNN capturan, de forma simple, varias ideas que sabemos del sistema visual biológico:\n",
    "\n",
    "1. Una **jerarquía** de procesamiento (de bordes locales a formas y objetos), análoga al recorrido V1 → V2 → V4 → IT\n",
    "2. **Campos receptivos** que crecen con la profundidad, como pasa en corteza visual\n",
    "\n",
    "En investigación, estas propiedades facilitan construir **modelos de codificación** de respuestas neuronales, comparar **representaciones internas** del modelo con actividad cerebral (p. ej., con RSA) y testear hipótesis sobre cómo se **extrae información visual** que luego guía la conducta.\n",
    "\n",
    "**¿Qué vamos a construir?**  \n",
    "\n",
    "Una CNN “en etapas” (inspirada en el flujo V1 → V2 → IT) con bloques `Conv2d → BatchNorm2d → ReLU`, *pooling* en las primeras etapas y un cierre con `AdaptiveAvgPool2d(1) → Linear`. Esta estructura:\n",
    "\n",
    "- **Estabiliza** el entrenamiento con `BatchNorm2d` (aprende más rápido y con LR razonable).\n",
    "- Es **agnóstica al tamaño** de entrada gracias a `AdaptiveAvgPool2d(1)` (no hay que calcular dimensiones para la capa final).\n",
    "- Mantiene la **intuición** de los filtros: las primeras capas detectan cambios locales (bordes, texturas); las siguientes combinan esas pistas.\n",
    "\n",
    "**Qué vamos a observar**\n",
    "\n",
    "1. **Baseline simple**: entrenar la red tal cual (inicialización por defecto) y medir la accuracy de validación.\n",
    "2. **Visualización**: inspeccionar los **mapas de activación** de la primera etapa sobre la imagen del perrito para ver qué regiones resaltan (por ejemplo, contornos de orejas y lomo).\n",
    "\n",
    "**Objetivo**\n",
    "\n",
    "Reforzar el puente entre los **kernels** trabajados a mano y una **CNN entrenable** que aprende a combinarlos para reconocer clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bloque de abajo implementa una red inspirada en **CORnet-Z**: organiza el flujo en etapas análogas a **V1 → V2 → IT**, donde cada etapa aplica `Conv2d → BatchNorm2d → ReLU` y las primeras dos incluyen *pooling* para aumentar el campo receptivo y ganar invariancia a traslación.\n",
    "\n",
    "- **V1 y V2**: detectan patrones locales (bordes/texturas) y reducen resolución con `MaxPool2d`.  \n",
    "- **IT**: combina rasgos de mayor escala con otra conv + BN (sin *pooling*) para preservar detalle.  \n",
    "- **Cabeza**: `AdaptiveAvgPool2d(1)` comprime cualquier tamaño a un vector por canal (**agnóstico al tamaño de entrada**) y `Linear(128, 10)` produce las clases.\n",
    "\n",
    "En el `forward` se guardan activaciones intermedias como `V1`, `V2` e `IT`, útiles para **visualizar mapas de activación** y discutir qué “ve” cada área del modelo (tal como se hace al analizar representaciones en neurociencia computacional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "      \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)\n",
    "    self.bn1   = nn.BatchNorm2d(64)\n",
    "    self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "    self.bn2   = nn.BatchNorm2d(128)\n",
    "    self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    self.conv3 = nn.Conv2d(128, 128, 3, padding=1, bias=False)\n",
    "    self.bn3   = nn.BatchNorm2d(128)\n",
    "    self.avg = nn.AdaptiveAvgPool2d(1) \n",
    "    self.fc   = nn.Linear(128, 10)  \n",
    "    \n",
    "  def forward(self, x):\n",
    "    V1 = F.relu(self.bn1(self.conv1(x)))\n",
    "    V1 = self.pool1(V1)\n",
    "    V2 = F.relu(self.bn2(self.conv2(V1)))\n",
    "    V2 = self.pool2(V2)\n",
    "    it = F.relu(self.bn3(self.conv3(V2)))\n",
    "    x = self.avg(it).flatten(1)\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el bloque que sigue entrenamos nuestra red sobre *Imagenette* y medimos su desempeño en validación. Definimos los *dataloaders* de train/val, elegimos **Adam** como optimizador y **CrossEntropyLoss** como función de pérdida, y fijamos semillas para mantener la **reproducibilidad**. El ciclo recorre varias **épocas**; en cada una entrenamos con `train_net(...)`, evaluamos con `eval_net(...)` y reportamos la **precisión** en validación.\n",
    "\n",
    "El objetivo es ver cómo evoluciona la *accuracy* a medida que la red aprende y, si hace falta, ajustar hiperparámetros (tamaño de batch, tasa de aprendizaje, número de épocas). Probá aumentando el número de épocas a 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = encontrar_device()\n",
    "\n",
    "cnn = SimpleCNN().to(device)\n",
    "\n",
    "# Achicamos los datasets para que sean mas rápido de entrenar\n",
    "transform = Compose([Grayscale(), Resize((64, 64)), ToTensor()])\n",
    "train_dataset = Imagenette(root='data', split=\"train\", size=\"160px\", transform=transform, download=False)\n",
    "val_dataset = Imagenette(root='data', split=\"val\", size=\"160px\", transform=transform, download=False)\n",
    "\n",
    "# Cargamos los datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Especificamos el optimizador\n",
    "optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "\n",
    "# Especificamos el criterio para medir la pérdida\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reproducibilidad\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Iteramos sobre una cierta cantidad de épocas\n",
    "tmax = 1\n",
    "for ep in range(1, tmax+1):\n",
    "  # Entrenamos a la red en el conjunto de entrenamiento\n",
    "  tr_loss, tr_acc = train_net(cnn, train_loader, optimizer_cnn, criterion)\n",
    "\n",
    "  # Evaluamos en el conjunto de evaluación\n",
    "  va_loss, va_acc = eval_net(cnn, val_loader, criterion)\n",
    "\n",
    "  print(f\"Época {ep:02d} | Precisión={va_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar los filtros aprendidos (primera capa)\n",
    "\n",
    "Para cerrar, miramos qué aprendió la primera capa. El comando de abajo dibuja los 64 kernels de `conv1` en una grilla. Fijate si aparecen patrones de borde (horizontales, verticales, diagonales), simetrías o texturas locales: eso conecta directamente con la intuición de las convoluciones 2D que trabajamos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizar_pesos(cnn.conv1, in_channel=0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
